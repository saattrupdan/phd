\documentclass[../../main]{subfiles}
\pagestyle{fancy}

\begin{document}

\chapter{Part II Introduction}
\thispagestyle{fancy}


\section{A virtual hypothesis}

\todo[inline]{Write introduction, history of the result etc}

\qquad Denote by $\di$ the theory\todo{Rephrase this in terms of generic embeddings?}
\eq{
  \text{$\zfc + \ch\ + $ there is an $\omega_{1}$-dense ideal on $\omega_{1}$}
}

and by $\di^+$ the theory
\eq{
  \text{$\zfc + \ch\ + $ there is an $\omega_{1}$-dense ideal on $\omega_{1}$ such that the induced generic}\\
    \text{embedding restricted to the ordinals is independent of the generic object.}
}

In this paper, we will give a full proof of the following result,

\begin{theorem}
  The following theories are equiconsistent:
  \begin{enumerate}
    \item $\zf + \ad_{\mathbb{R}} + \Theta$ is regular,
    \item $\di^+$
  \end{enumerate}
\end{theorem}

Most of our results don't require the full strength of $\di^+$ and we've stated the needed assumptions in these cases, but the reader may simply assume $\di^+$ for the remainder of this paper if they wish to.


\section{Core models}

As we will be utilising the \textit{core model induction} to prove the lower consistency bounds of our hypothesis $\di^+$, we give here an idea of what we mean by the \textit{core model}. A convenient feature of core model theory is that most of the technical details regarding the construction is not needed for applications; it simply suffices to know only its abstract properties. We will provide a glimpse of the construction at the end of this section, as this will be useful when we create variants of the core model in Chapter \ref{chapter.internal-core-model-induction}. To see the full construction we refer the interested reader to \cite{MSc}, \cite{Zeman} and \cite{Kwithoutmeasurable}. 

\qquad The \textbf{core model}\footnote{$K$ is short for \textit{Kern}, meaning \textit{core} in German.} $K$ of a universe is the roughly speaking the subuniverse that strikes a balance between retaining the complexity of the universe while being as simple as possible. The problem is then making all of this precise. Some aspects of the definition is agreed upon by most researchers:
\begin{enumerate}
  \item We choose to define the \textit{complexity} of a universe by its large cardinal structure. This is based on the empirical fact that large cardinals seem to capture the strength of every ``naturally defined'' hypothesis, and gives us a convenient yard stick. For instance, a universe containing a measurable cardinal is more complex than $L$, as Scott's Theorem, see \cite[Corollary 5.5]{Kanamori}, shows that $L$ cannot contain any measurable cardinals (or any large cardinals stronger than measurables);
  \item We further postulate that $L$ is the simplest universe there is, and the simplicity of a universe should therefore be measured in terms of how much it resembles $L$. We will be more precise about what it means to ``resemble $L$'' below, but with this intuitive notion is should at least be clear that, say, $L$ is simpler than $L[\mu]$.\\
\end{enumerate}

Even though $(i)$ captures what we mean by complexity, it leaves much to be desired. For instance, as the structure of the large cardinal hierarchy can only be verified empirically, we might end up in an unfortunate situation where we simply do not know whether a given universe is more complex than another one\footnote{It might also be the case that the large cardinal hierarchy is not linear at all.}. The famous example of this is the current situation with the superstrong- and strongly compact cardinals, that we simply do not know which one is stronger\footnote{Although the general consensus is that the strongly compact cardinals should be equiconsistent with the supercompacts, making them stronger than the superstrongs.}. Thus, given a universe whose strength corresponds to that of a strongly compact and another one at the level of superstrongs, we would not be able to say which one is more complex. 

\qquad To remedy this unfortunate situation, we choose instead to define the complexity of a universe in terms of an intermediate property. A universe satisfying this property should then entail that it inherits the large cardinal structure of its surrounding universe. All the intermediate properties currently being used are all instances of a general phenomenon called \textit{covering}. The intuitive idea is that every set in the universe can be ``approximated'' by a set in the subuniverse, and arose from a seminal theorem of Jensen, see \cite[Theorem 11.56]{SchindlerBook}, stating that $0^\sharp$ exists if and only if \textit{strong covering} fails for $L$, defined as follows.

\defi[Jensen]{
  We say that \textbf{strong covering} holds for universes $\U\subset\V$ if to every $\alpha<o(\V)$ and $X\in\p^{\V}(\alpha)$ there exists $A\in\U$ such that $X\subset A$ and $\Card^{\V}(X) = \Card^{\V}(A)$.
}

We can then interpret Jensen's result as saying that, if the complexity of the surrounding universe $\V$ is below the strength of $0^\sharp$ then $L$ is a good candidate for $K$. In a complex universe we would therefore be looking for the core model among subuniverses more complex than $L$, and it turns out that also requiring strong covering to hold in such models is too much to ask; the current definition of covering has thus been weakened to the following.

\defi{
  We say that \textbf{(weak) covering} holds for universes $\U\subset\V$ if $\cof^{\V}(\alpha^{+\U}) = \Card^{\V}(\alpha^{+\U})$ holds for any ordinal $\alpha$ with $\alpha^{+\U}\geq\aleph_2^{\V}$.
}

This statement might seem very distant from the strong version, but one can think of weak covering as saying that $\U$ ``knows'' the true cofinality of its successor cardinals $\kappa\geq\aleph_2^{\V}$ within the error margin $\varepsilon:=\kappa^{+\U}-\Card^{\V}(\kappa)$. More concretely, we could equivalently define weak covering as $\U$ containing all cofinal maps $f\colon\gamma\to\kappa$ in $\V$ for every $\gamma\in\Card^{\V}(\kappa)$, making it closer in spirit to the strong covering property.

\figx[Weak covering property]{weak-covering.jpg}

\qquad When it comes to $(ii)$ we have to define what we mean by ``resembling $L$''. Ultimately this boils down to the current working definition of a \textit{mouse} and is still work in progress. If our universe is no more complex than the strength of a Woodin cardinal however, then we know what the correct definition of a mouse is, and hence also what ``resembling $L$'' would mean in this context. The definition of mice along with the assumption of covering then turns out to imply that the core model will indeed inherit the large cardinal strength of the universe\footnote{To show this one first uses covering to show that $K$ is \textit{universal}, i.e. that it wins every coiteration. With universality at hand, a comparison argument with any $L[\vec E]$-model containing a large cardinal will then show that $K$ will have an inner model with the large cardinal in question.}.

\qquad To construct the core model one could then take a bottom-up approach, starting with $L$ and then carefully include the complexity of the universe while remaining similar to $L$\footnote{This is the strategy undertaken by Steel and Sargsyan.}. Alternatively, a top-down approach would be to define a structure which has \textit{all} the complexity of the universe, and then showing that this structure indeed exhibits these $L$-like properties\footnote{Woodin is pursuing this path.}.

\qquad The construction of $K$ takes the bottom-up approach. The first step towards this is the construction of $K^c$\footnote{The ``c'' stands for \textit{certified}, as the extenders we put on the sequence was historically called \textit{certified extenders}.}, which we build by recursion on the ordinals. We start with $K^c_0 := \emptyset$ and at every successor ordinal $\alpha$ we do one of two things:
\begin{enumerate}
  \item If there exists a ``nice'' extender indexed at $\alpha$ then we put it onto the extender sequence of $\core(K^c_\alpha$), where $\core(X)$ is the transitive collapse of a certain hull of $X$\footnote{Think of $\core(X)$ as ``removing the noise of $X$''.};
  \item Otherwise we let $K^c_\alpha := \J(\core(K^c_{\alpha-1}))$, with $\J(x):=\text{rud}(\trcl(x\cup\{x\}))$ being the usual operator we use to build $L$ with Jensen's hierarchy.\\
\end{enumerate}

In other words, we are essentially building $L$ with extenders attached onto it in a canonical fashion. Taking cores at every step will ensure that the initial segments will be \textit{sound}, which ultimately is what guarantees iterability of $K^c$. The fact that we put on all the relevant extenders from $V$ is what will ensure the covering property of the model. It turns out that $K^c$ isn't exactly what we want however, as it relies \textit{too much} on the surrounding universe, in contrast with $L$ whose construction procedure builds the exact same model in every universe. To attain this \textit{canonicity} we are again taking certain ``thick'' hulls of $K^c$ (again, think of it as removing the noise). The resulting construction \textit{almost} gives us what we want and is dubbed \textit{pseudo-K}. The problem with this is that the technicalities of the construction uses certain properties of a fixed cardinal $\Omega$, so to build the true core model we ``glue'' these pseudo-K's together.

\qquad The takeaway here is that whenever we're working with an initial segment of $K$ then that segment will be build using the recursive steps $(i)$ and $(ii)$ above, carefully including extenders from $V$.


\section{Mice and games}

\todo[inline]{Define mice, $M_n^\sharp(x)$, iteration game, exit extender, cutpoint, condenses well, relativises well}


\section{Core model induction}

Before we start with the actual induction, this section will attempt to give the reader an overview of what's going to happen, which will hopefully make it easier to understand the lemmata along the way to the finish line.

\qquad A core model induction is a way of producing determinacy models from strong hypotheses. What we're trying to do is to show that many subsets of the reals are determined, so one place to start could be the projective hierarchy, as Martin has shown that $\zfc$ alone proves that all the Borel sets are determined.

\qquad To show that the projective sets are determined, we use the M\" uller-Neeman-Woodin result that $\bf\Sigma^1_{n+1}$-determinacy is equivalent to the existence and iterability of $M_n^\sharp(x)$ for every real $x$. So from our given hypothesis we then, somehow, manage to show that all these mice exist, giving us projective determinacy.

\qquad A next step could then be to notice that the projective sets of reals are precisely those reals belonging to $J_1(\mathbb R)$, so we would then want to show that \textit{all} the sets of reals in $L(\mathbb R)$ are determined, by an induction on the levels. Kechris-Woodin-Solovay \todo{Check authors.} shows that we only need to check that the sets of reals in $J_{\alpha+1}(\mathbb R)$ for so-called \textit{critical} ordinals $\alpha$ are determined.

\qquad This is convenient, since Steel (see \cite{scalesinL(R)}) has characterised these critical ordinals and showed that they fall into a handful of cases, the notable ones being the \textit{inadmissible case} and the \textit{end of gap case}. Long story short, the so-called \textit{Witness Equivalence} shows that to prove $J_{\alpha+1}(\mathbb R)\models\ad$ for a critical ordinal $\alpha$, it suffices to show that $M_n^F(x)$ exists and is iterable for a certain operator $F$, in analogy with what happened with the projective sets.

\qquad This part of the induction, showing $\ad^{L{(\mathbb R)}}$, is an instance of an \textit{internal} core model induction: we're showing that all the sets of reals in some fixed inner model are determined. Crucially, for these internal core model inductions to work, we need a \textit{scale analysis} of the model at hand. In this paper we will be working with the \textit{lower part model} $\lp(\mathbb R)$, which contains all the sets of reals of $L(\mathbb R)$ and more, and generalisations of such lower part models. The scale analysis of $\lp(\mathbb R)$ is shown in Steel \todo{Reference Scales in $K(\mathbb R)$ and the companion paper.}, and the scale analysis for the generalised versions is shown in Trang and Schlutzenberg \todo{Reference their scale paper.}.

\qquad Our first internal step will thus show that every set of reals in $\lp(\mathbb R)$ is determined, which consistency-wise is a tad stronger than having a limit of Woodin cardinals. This first step can be seen as showing that all sets of reals in the pointclass $(\Sigma^2_1)^{\lp(\mathbb R)}$ \todo{Boldface?} are determined, since being in this pointclass precisely means that you belong to an initial segment of $\lp(\mathbb R)$.

\qquad The \textit{external} core model induction takes this further. If we define $\Gamma_\infty$ to be the set of all determined sets of reals, we want to see how big this pointclass is. We organise this by looking at the so-called \textit{Solovay sequence} $\bra{\theta_\alpha\mid\alpha\leq\Omega}$ of $L(\Gamma_\infty,\mathbb R)$, whose length can be seen as a measure of ``how many determined sets of reals there are'' in a context without the axiom of choice.

\qquad If $\Omega=0$ then it can be shown that $L(\Gamma_\infty,\mathbb R)$ and $\lp(\mathbb R)$ have the same sets of reals\todo{Even equal I think}, so if we want to show that $\Omega>0$ then it suffices to find some determined set of reals which is not in $\lp(\mathbb R)$. This is done by producing a so-called $(\Sigma^2_1)^{L(\Gamma_\infty,\mathbb R)}$-fullness preserving hod pair $(\P_0,\Lambda_0)$, which will have the property that $\Lambda_0\notin\lp(\mathbb R)$ and that $\Lambda_0$ is determined when viewed as a set of reals. This yields a contradiction to $\Omega=0$, so we must have that $\Omega>0$.

\qquad Next, if we assume that $\Omega=1$ then we show that $L(\Gamma_\infty,\mathbb R)$ and $\lp^{\Lambda_0}(\mathbb R)$ have the same sets of reals, where $\lp^{\Lambda_0}(\mathbb R)$ is one of the generalised versions of $\lp(\mathbb R)$ we mentioned above. We do another internal induction to show that every set of reals in $\lp^{\Lambda_0}(\mathbb R)$ is determined, and then proceed to construct a $\Sigma^2_1(\Lambda_0)^{L(\Gamma_\infty,\mathbb R)}$-fullness preserving hod pair $(\P_1,\Lambda_1)$, which again has the property that $\Lambda_1\notin\lp^{\Lambda_0}(\mathbb R)$, so that we must have $\Omega>1$.

\qquad Now assume that $\Omega=2$ --- this step will look like the general \textit{successor case}. This time we're working with $\lp^{\Gamma_0,\Lambda_1}(\mathbb R)$, where $\Gamma_0:=\Gamma(\P_0,\Lambda_0)$ is a pointclass generated by $(\P_0,\Lambda_0)$. We again produce a $\Sigma^2_1(\Lambda_1)^{\lp^{\Gamma_0,\Lambda_1}(\mathbb R)}$-fullness preserving hod pair $(\P_2,\Lambda_2)$ with $\Lambda_2\notin\lp^{\Gamma_0,\Lambda_1}(\mathbb R)$, showing that $\Omega>2$.

\qquad As for the limit case, if we assume that $\Omega=\gamma$, we let $\Gamma_\gamma:=\bigcup_{\alpha<\gamma}\Gamma_\alpha$ and coiterate all the previous mice to get some $\P_\gamma$, which we then have to show has a $\Sigma^2_1(\Lambda_\gamma)^{\lp^{\Gamma_\gamma,\oplus_{\alpha<\gamma}\Lambda_\alpha}}$-fullness preserving iteration strategy $\Lambda_\gamma$. As before, this strategy will be determined as a set of reals and won't be in $L(\Gamma_\infty,\mathbb R)$, a contradiction, which shows that $\Omega>\gamma$.

\qquad In this paper, we will be able to do the tame case, the successor case, and the limit case when $\Omega$ is singular, which shows that we end up getting that $\Omega$ is regular.


\end{document}
